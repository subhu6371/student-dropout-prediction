{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e91c6512-a778-4523-b0f3-bf38050f42ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wbgapi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwbgapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwb\u001b[39;00m \u001b[38;5;66;03m# You may need to run: pip install wbgapi\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# --- 1. Enrich Data with World Bank Economic Data ---\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# The dataset is from Portugal (ISO code 'PRT') around 2015\u001b[39;00m\n\u001b[0;32m      7\u001b[0m country_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRT\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wbgapi'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import wbgapi as wb # You may need to run: pip install wbgapi\n",
    "\n",
    "# --- 1. Enrich Data with World Bank Economic Data ---\n",
    "\n",
    "# The dataset is from Portugal (ISO code 'PRT') around 2015\n",
    "country_code = \"PRT\"\n",
    "year = 2015\n",
    "# The indicator for GDP per capita\n",
    "indicator = \"NY.GDP.PCAP.CD\"\n",
    "\n",
    "try:\n",
    "    # Fetch the GDP per capita from the World Bank API\n",
    "    gdp_per_capita = wb.data.get_data(country_code, {indicator}, time=year)[indicator]\n",
    "    print(f\"Successfully fetched GDP per capita for Portugal in {year}: ${gdp_per_capita:,.2f}\")\n",
    "\n",
    "    # Add this new economic feature to our main dataframe\n",
    "    # This represents the general economic condition for all students\n",
    "    final_df['gdp_per_capita'] = gdp_per_capita\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not fetch data, using an approximate value. Error: {e}\")\n",
    "    # If the API fails for any reason, we have a backup value\n",
    "    final_df['gdp_per_capita'] = 20000 # Approximate value for Portugal in 2015\n",
    "\n",
    "print(\"\\nAdded 'gdp_per_capita' feature to the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aaf64f0-619d-4d8d-894e-984d6b93a59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wbgapi\n",
      "  Downloading wbgapi-1.0.12-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from wbgapi) (2.32.3)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from wbgapi) (6.0.2)\n",
      "Requirement already satisfied: tabulate in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from wbgapi) (0.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from requests->wbgapi) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from requests->wbgapi) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from requests->wbgapi) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from requests->wbgapi) (2025.1.31)\n",
      "Downloading wbgapi-1.0.12-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: wbgapi\n",
      "Successfully installed wbgapi-1.0.12\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wbgapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9001d47b-efd4-4e43-8856-b5472524e3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data preparation complete.\n",
      "\n",
      "Could not fetch data, using an approximate value. Error: module 'wbgapi.data' has no attribute 'get_series'\n",
      "\n",
      "Added 'gdp_per_capita' feature to the dataset.\n",
      "   age  failures  gdp_per_capita  at_risk\n",
      "0   15       1.0           20000        0\n",
      "1   15       1.0           20000        1\n",
      "2   15       2.0           20000        1\n",
      "3   15       0.0           20000        0\n",
      "4   15       0.0           20000        1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import wbgapi as wb  # <-- Make sure you run: pip install wbgapi\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "# --- Step 1: Data Loading and Preparation ---\n",
    "# This part loads and prepares the 'final_df' dataframe first.\n",
    "mat_df = pd.read_csv('student-mat.csv', sep=';')\n",
    "por_df = pd.read_csv('student-por.csv', sep=';')\n",
    "merge_keys = [\"school\", \"sex\", \"age\", \"address\", \"famsize\", \"Pstatus\", \"Medu\", \"Fedu\",\n",
    "              \"Mjob\", \"Fjob\", \"reason\", \"nursery\", \"internet\"]\n",
    "df_merged = pd.merge(mat_df, por_df, on=merge_keys, how='outer', suffixes=('_math', '_por'))\n",
    "for col in ['G1', 'G2', 'G3', 'absences', 'failures']:\n",
    "    df_merged[col] = df_merged[col + '_math'].fillna(df_merged[col + '_por'])\n",
    "df_merged['at_risk'] = (df_merged['G3'] < 10).astype(int)\n",
    "final_columns = merge_keys + ['traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid',\n",
    "                              'activities', 'higher', 'romantic', 'famrel', 'freetime', 'goout',\n",
    "                              'Dalc', 'Walc', 'health', 'absences', 'at_risk']\n",
    "final_df = df_merged[merge_keys].copy()\n",
    "for col in final_columns:\n",
    "    if col in merge_keys or col == 'at_risk': continue\n",
    "    final_df[col] = df_merged[col + '_math'].fillna(df_merged[col + '_por'])\n",
    "final_df['at_risk'] = df_merged['at_risk']\n",
    "print(\"Initial data preparation complete.\")\n",
    "\n",
    "\n",
    "# --- Step 2: Enrich Data with World Bank Economic Data ---\n",
    "# This block now runs after 'final_df' is created, fixing the NameError.\n",
    "country_code = \"PRT\"\n",
    "year = 2015\n",
    "indicator = \"NY.GDP.PCAP.CD\"\n",
    "\n",
    "try:\n",
    "    # FIXED: The function name has been corrected to wb.data.get_series()\n",
    "    year_str = str(year)\n",
    "    gdp_series = wb.data.get_series(indicator, country_code, id_or_value='id', simplify_index=True)\n",
    "    gdp_per_capita = gdp_series[year_str]\n",
    "    \n",
    "    print(f\"\\nSuccessfully fetched GDP per capita for Portugal in {year}: ${gdp_per_capita:,.2f}\")\n",
    "    final_df['gdp_per_capita'] = gdp_per_capita\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not fetch data, using an approximate value. Error: {e}\")\n",
    "    final_df['gdp_per_capita'] = 20000\n",
    "\n",
    "print(\"\\nAdded 'gdp_per_capita' feature to the dataset.\")\n",
    "print(final_df[['age', 'failures', 'gdp_per_capita', 'at_risk']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77fc918a-ee34-4c24-851f-5a63a2d37122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data preparation complete.\n",
      "\n",
      "Successfully fetched GDP per capita for Portugal in 2015: $19,215.78\n",
      "\n",
      "Added 'gdp_per_capita' feature to the dataset.\n",
      "   age  failures  gdp_per_capita  at_risk\n",
      "0   15       1.0    19215.781301        0\n",
      "1   15       1.0    19215.781301        1\n",
      "2   15       2.0    19215.781301        1\n",
      "3   15       0.0    19215.781301        0\n",
      "4   15       0.0    19215.781301        1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests  # We will use this library directly\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "# --- Step 1: Data Loading and Preparation ---\n",
    "mat_df = pd.read_csv('student-mat.csv', sep=';')\n",
    "por_df = pd.read_csv('student-por.csv', sep=';')\n",
    "merge_keys = [\"school\", \"sex\", \"age\", \"address\", \"famsize\", \"Pstatus\", \"Medu\", \"Fedu\",\n",
    "              \"Mjob\", \"Fjob\", \"reason\", \"nursery\", \"internet\"]\n",
    "df_merged = pd.merge(mat_df, por_df, on=merge_keys, how='outer', suffixes=('_math', '_por'))\n",
    "for col in ['G1', 'G2', 'G3', 'absences', 'failures']:\n",
    "    df_merged[col] = df_merged[col + '_math'].fillna(df_merged[col + '_por'])\n",
    "df_merged['at_risk'] = (df_merged['G3'] < 10).astype(int)\n",
    "final_columns = merge_keys + ['traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid',\n",
    "                              'activities', 'higher', 'romantic', 'famrel', 'freetime', 'goout',\n",
    "                              'Dalc', 'Walc', 'health', 'absences', 'at_risk']\n",
    "final_df = df_merged[merge_keys].copy()\n",
    "for col in final_columns:\n",
    "    if col in merge_keys or col == 'at_risk': continue\n",
    "    final_df[col] = df_merged[col + '_math'].fillna(df_merged[col + '_por'])\n",
    "final_df['at_risk'] = df_merged['at_risk']\n",
    "print(\"Initial data preparation complete.\")\n",
    "\n",
    "\n",
    "# --- Step 2: Enrich Data with World Bank API (Direct Method) ---\n",
    "country_code = \"PRT\"\n",
    "year = 2015\n",
    "indicator = \"NY.GDP.PCAP.CD\"\n",
    "\n",
    "# Construct the API URL\n",
    "api_url = f\"http://api.worldbank.org/v2/country/{country_code}/indicator/{indicator}?date={year}&format=json\"\n",
    "\n",
    "try:\n",
    "    # Make the direct API request\n",
    "    response = requests.get(api_url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Check if the response contains valid data and extract the value\n",
    "    if data and len(data) > 1 and data[1]:\n",
    "        gdp_per_capita = data[1][0]['value']\n",
    "        print(f\"\\nSuccessfully fetched GDP per capita for Portugal in {year}: ${gdp_per_capita:,.2f}\")\n",
    "        final_df['gdp_per_capita'] = gdp_per_capita\n",
    "    else:\n",
    "        # Fallback if the API returns an empty response\n",
    "        print(\"\\nAPI returned no data, using an approximate value.\")\n",
    "        final_df['gdp_per_capita'] = 20000\n",
    "\n",
    "except Exception as e:\n",
    "    # Fallback if the request fails (e.g., no internet)\n",
    "    print(f\"\\nCould not fetch data, using an approximate value. Error: {e}\")\n",
    "    final_df['gdp_per_capita'] = 20000\n",
    "\n",
    "print(\"\\nAdded 'gdp_per_capita' feature to the dataset.\")\n",
    "print(final_df[['age', 'failures', 'gdp_per_capita', 'at_risk']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfebeb20-6bc7-4181-a1b4-4a7ee36039ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not retrieve air quality data.\n",
      "\n",
      "Successfully fetched air quality data.\n",
      "Number of days with 'Fair' or worse air quality: 0\n",
      "\n",
      "Added 'poor_air_quality_days' feature to the dataset.\n",
      "   age  failures  gdp_per_capita  poor_air_quality_days  at_risk\n",
      "0   15       1.0    19215.781301                      0        0\n",
      "1   15       1.0    19215.781301                      0        1\n",
      "2   15       2.0    19215.781301                      0        1\n",
      "3   15       0.0    19215.781301                      0        0\n",
      "4   15       0.0    19215.781301                      0        1\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Enrich Data with Air Quality API ---\n",
    "\n",
    "def get_poor_air_quality_days(latitude, longitude, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Uses the Open-Meteo Air Quality API to count days where the European Air Quality Index (AQI) was poor.\n",
    "    An AQI > 2 is generally considered 'Fair' or 'Moderate', so we'll count those days.\n",
    "    \"\"\"\n",
    "    api_url = \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"daily\": \"european_aqi\",\n",
    "        \"timezone\": \"Europe/London\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(api_url, params=params)\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'daily' not in data:\n",
    "            print(\"Could not retrieve air quality data.\")\n",
    "            return 0\n",
    "            \n",
    "        df = pd.DataFrame(data['daily'])\n",
    "        \n",
    "        # Count the number of days where the European AQI was 2 or higher (not 'Good')\n",
    "        poor_air_quality_days = df[df['european_aqi'] >= 2].shape[0]\n",
    "        return poor_air_quality_days\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while fetching air quality data: {e}\")\n",
    "        return 0 # Return 0 if the API call fails\n",
    "\n",
    "# The same school location as before\n",
    "school_lat = 38.52\n",
    "school_lon = -8.01\n",
    "school_year_start = \"2015-09-01\"\n",
    "school_year_end = \"2016-06-30\"\n",
    "\n",
    "# Get the number of poor air quality days\n",
    "num_poor_aqi_days = get_poor_air_quality_days(school_lat, school_lon, school_year_start, school_year_end)\n",
    "\n",
    "print(f\"\\nSuccessfully fetched air quality data.\")\n",
    "print(f\"Number of days with 'Fair' or worse air quality: {num_poor_aqi_days}\")\n",
    "\n",
    "# Add this new, powerful feature to our main dataframe\n",
    "final_df['poor_air_quality_days'] = num_poor_aqi_days\n",
    "\n",
    "print(\"\\nAdded 'poor_air_quality_days' feature to the dataset.\")\n",
    "print(final_df[['age', 'failures', 'gdp_per_capita', 'poor_air_quality_days', 'at_risk']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e867f943-ed4b-4869-866e-326d162e238b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading tweepy-4.16.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting textblob\n",
      "  Using cached textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting oauthlib<4,>=3.2.0 (from tweepy)\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from tweepy) (2.32.3)\n",
      "Collecting requests-oauthlib<3,>=1.2.0 (from tweepy)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Downloading tweepy-4.16.0-py3-none-any.whl (98 kB)\n",
      "Using cached textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy, textblob\n",
      "Successfully installed oauthlib-3.3.1 requests-oauthlib-2.0.0 textblob-0.19.0 tweepy-4.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tweepy textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21b7b932-4400-4190-b85c-8387f8c5378f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while connecting to the X API: 429 Too Many Requests\n",
      "Too Many Requests\n",
      "Proceeding with a neutral sentiment score.\n",
      "\n",
      "Average Education Sentiment Score calculated: 0.0000\n",
      "\n",
      "Added 'avg_education_sentiment' feature to the dataset.\n",
      "   age  failures  gdp_per_capita  poor_air_quality_days  \\\n",
      "0   15       1.0    19215.781301                      0   \n",
      "1   15       1.0    19215.781301                      0   \n",
      "2   15       2.0    19215.781301                      0   \n",
      "3   15       0.0    19215.781301                      0   \n",
      "4   15       0.0    19215.781301                      0   \n",
      "\n",
      "   avg_education_sentiment  at_risk  \n",
      "0                      0.0        0  \n",
      "1                      0.0        1  \n",
      "2                      0.0        1  \n",
      "3                      0.0        0  \n",
      "4                      0.0        1  \n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- Step 4: Enrich Data with X API Sentiment Analysis ---\n",
    "\n",
    "# IMPORTANT: Replace this with your actual Bearer Token from the X Developer Portal\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAADO34gEAAAAAe6rM4nHnxsSq2cobWlnuqvLtQ1k%3D3ZRDCihdXnfFbfoMGmu4VvuxwbLkBYsaReETrz9624Cm6D5s4N\"\n",
    "\n",
    "# The location of the schools is in Portugal, so we search for tweets in Portuguese\n",
    "# We look for keywords related to education during the school year\n",
    "query = \"(escola OR exame OR estudar) lang:pt -is:retweet\" # (school OR exam OR study) in Portuguese, no retweets\n",
    "start_time = \"2015-09-01T00:00:00Z\"\n",
    "end_time = \"2016-06-30T23:59:59Z\"\n",
    "\n",
    "# Initialize a list to store sentiment scores\n",
    "sentiment_scores = []\n",
    "avg_education_sentiment = 0.0 # Default to neutral\n",
    "\n",
    "try:\n",
    "    # Authenticate with the X API\n",
    "    client = tweepy.Client(bearer_token)\n",
    "    \n",
    "    # Search for recent tweets that match our query.\n",
    "    # NOTE: The free X API has very strict limits on searching for old tweets.\n",
    "    # This may return very few or no results. This is a known limitation.\n",
    "    response = client.search_recent_tweets(query, start_time=start_time, end_time=end_time, max_results=100)\n",
    "    \n",
    "    # Check if the API returned any tweets\n",
    "    if response.data:\n",
    "        for tweet in response.data:\n",
    "            # Use TextBlob to get the sentiment polarity (-1 for negative, +1 for positive)\n",
    "            blob = TextBlob(tweet.text)\n",
    "            sentiment_scores.append(blob.sentiment.polarity)\n",
    "            \n",
    "        # Calculate the average sentiment if we found any tweets\n",
    "        if sentiment_scores:\n",
    "            avg_education_sentiment = np.mean(sentiment_scores)\n",
    "            print(f\"Successfully fetched {len(sentiment_scores)} tweets.\")\n",
    "    else:\n",
    "        print(\"No tweets found for the given query and time range. This is common with the free API tier.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while connecting to the X API: {e}\")\n",
    "    print(\"Proceeding with a neutral sentiment score.\")\n",
    "\n",
    "# Add the new feature to our main dataframe\n",
    "final_df['avg_education_sentiment'] = avg_education_sentiment\n",
    "\n",
    "print(f\"\\nAverage Education Sentiment Score calculated: {avg_education_sentiment:.4f}\")\n",
    "print(\"\\nAdded 'avg_education_sentiment' feature to the dataset.\")\n",
    "print(final_df[['age', 'failures', 'gdp_per_capita', 'poor_air_quality_days', 'avg_education_sentiment', 'at_risk']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "320d251b-d191-43c7-89f9-aa9b481e676b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytrends\n",
      "  Downloading pytrends-4.9.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.0 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from pytrends) (2.32.3)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from pytrends) (2.2.3)\n",
      "Requirement already satisfied: lxml in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from pytrends) (5.3.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from pandas>=0.25->pytrends) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from pandas>=0.25->pytrends) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from pandas>=0.25->pytrends) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from pandas>=0.25->pytrends) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from requests>=2.0->pytrends) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from requests>=2.0->pytrends) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from requests>=2.0->pytrends) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from requests>=2.0->pytrends) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends) (1.17.0)\n",
      "Downloading pytrends-4.9.2-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: pytrends\n",
      "Successfully installed pytrends-4.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytrends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c1b48fc-6f80-4f89-b3d5-f028c0a748a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pausing for 60 seconds before making Google Trends request...\n",
      "Resuming...\n",
      "An error occurred with Google Trends: The request failed: Google returned a response with code 429\n",
      "Proceeding with a default value for the stress score.\n",
      "\n",
      "Average Exam Stress Score (from Google Trends): 0.00\n",
      "\n",
      "Added 'exam_stress_score' feature to the dataset.\n"
     ]
    }
   ],
   "source": [
    "from pytrends.request import TrendReq\n",
    "import pandas as pd\n",
    "import time # <-- 1. Import the time library\n",
    "\n",
    "# --- Step 4 (Revised & Robust): Enrich Data with Google Trends API ---\n",
    "\n",
    "# Initialize the Google Trends API\n",
    "pytrends = TrendReq(hl='pt-PT', tz=0)\n",
    "\n",
    "# Define our keywords and timeframe\n",
    "keywords = [\"exame\"]\n",
    "start_date = \"2015-09-01\"\n",
    "end_date = \"2016-06-30\"\n",
    "timeframe = f'{start_date} {end_date}'\n",
    "\n",
    "avg_exam_stress_score = 0.0 # Default to 0\n",
    "\n",
    "try:\n",
    "    # 2. Add a \"polite pause\" to avoid getting blocked by Google\n",
    "    print(\"Pausing for 60 seconds before making Google Trends request...\")\n",
    "    time.sleep(60)\n",
    "    print(\"Resuming...\")\n",
    "\n",
    "    # Build the payload\n",
    "    pytrends.build_payload(keywords, cat=0, timeframe=timeframe, geo='PT')\n",
    "\n",
    "    # Get the interest over time data\n",
    "    interest_over_time_df = pytrends.interest_over_time()\n",
    "\n",
    "    if not interest_over_time_df.empty:\n",
    "        avg_exam_stress_score = interest_over_time_df['exame'].mean()\n",
    "        print(f\"Successfully fetched Google Trends data.\")\n",
    "        interest_over_time_df.plot(title='Search Interest for \"Exame\" in Portugal (2015-2016)')\n",
    "    else:\n",
    "        print(\"Could not fetch Google Trends data for the specified timeframe.\")\n",
    "\n",
    "except Exception as e:\n",
    "    # 3. Add a fallback in case the error happens again\n",
    "    print(f\"An error occurred with Google Trends: {e}\")\n",
    "    print(\"Proceeding with a default value for the stress score.\")\n",
    "\n",
    "\n",
    "# Add this new, powerful feature to our main dataframe\n",
    "final_df['exam_stress_score'] = avg_exam_stress_score\n",
    "\n",
    "print(f\"\\nAverage Exam Stress Score (from Google Trends): {avg_exam_stress_score:.2f}\")\n",
    "print(\"\\nAdded 'exam_stress_score' feature to the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0ec8d26-40d0-45d2-81e5-5d8d236d3360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the advanced Stacking Ensemble model... Please wait.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [23:26:15] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [23:26:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [23:26:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [23:26:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [23:26:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\subha\\anaconda3\\envs\\anaconda-2025.04-py3.11\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [23:26:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "\n",
      "--- Ultimate Stacking Model Evaluation Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.88      0.82        96\n",
      "           1       0.56      0.37      0.44        41\n",
      "\n",
      "    accuracy                           0.72       137\n",
      "   macro avg       0.66      0.62      0.63       137\n",
      "weighted avg       0.70      0.72      0.70       137\n",
      "\n",
      "\n",
      "✅ Ultimate stacking model saved to 'ultimate_stacking_model.pkl' successfully!\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "# --- Step 5: Build and Train the Ultimate \"Stacking\" Model ---\n",
    "\n",
    "# Prepare our final, fully enriched dataset for modeling\n",
    "# We drop 'at_risk' to create our features (X) and use it as our target (y)\n",
    "X_final = final_df.drop('at_risk', axis=1) \n",
    "y_final = final_df['at_risk']\n",
    "\n",
    "# Apply One-Hot Encoding to convert any remaining text columns to numbers\n",
    "X_final = pd.get_dummies(X_final, drop_first=True)\n",
    "\n",
    "# Split the data for a final, fair evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.2, random_state=42, stratify=y_final)\n",
    "\n",
    "# 1. Define the \"expert\" models that will learn from the data\n",
    "# We'll use a Random Forest and a tuned XGBoost model.\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('xgb', xgb.XGBClassifier(\n",
    "        objective='binary:logistic', \n",
    "        eval_metric='logloss', \n",
    "        use_label_encoder=False, \n",
    "        random_state=42,\n",
    "        scale_pos_weight=3  # This tells XGBoost to focus on finding at-risk students\n",
    "    ))\n",
    "]\n",
    "\n",
    "# 2. Define the \"manager\" model that learns from the experts\n",
    "# Logistic Regression is a great choice for the final manager.\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=estimators, \n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5 # Use cross-validation for robustness\n",
    ")\n",
    "\n",
    "# 3. Train the entire team of models! This may take a minute.\n",
    "print(\"\\nTraining the advanced Stacking Ensemble model... Please wait.\")\n",
    "stacking_model.fit(X_train, y_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# 4. Evaluate the ultimate model's performance on unseen data\n",
    "print(\"\\n--- Ultimate Stacking Model Evaluation Report ---\")\n",
    "y_pred_ultimate = stacking_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_ultimate))\n",
    "\n",
    "# 5. Save the final, most powerful model for your Streamlit app\n",
    "joblib.dump(stacking_model, 'ultimate_stacking_model.pkl')\n",
    "print(\"\\n✅ Ultimate stacking model saved to 'ultimate_stacking_model.pkl' successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e035896-2426-4b64-9d41-dc99ac387ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2025.04-py3.11",
   "language": "python",
   "name": "anaconda-2025.04-py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
